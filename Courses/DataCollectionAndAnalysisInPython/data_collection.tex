\documentclass{article}
\usepackage[paper=a4paper, top=20mm, bottom=15mm,left=20mm,right=15mm]{geometry}

% math
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

% Russian
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

% for double images
\usepackage{subcaption}

% Used for img import
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}

\newcommand{\incfig}[2][1]{%
    \def\svgwidth{#1\columnwidth}
    \import{./figures/}{#2.pdf_tex}
}

\pdfsuppresswarningpagegroup=1

\title{Сбор и анализ данных в Python}
\date{2021}
\author{Seara}
\pagenumbering{gobble}

\begin{document}
\maketitle
\newpage
\pagenumbering{arabic}

\section{Week 1}
\subsection{Основные понятия теории вероятностей}
\paragraph{Пакт.}
$X,Y,Z$ - случайные величины
\\
$x,y,z$ - какие-то конкретные значения
\\
$A,B,C$ - какие-то события
\\
$\mathbb{P}$ - вероятность
\\
$\E(X)$ - мат. ожидание
\\
$Var(X)$ - дисперсия
\\
$Cov(X,Y), \rho(X,Y)$ - ковариация и корреляция
\subsection{Дискретные случайные величины}
\paragraph{Случайная величина и её распределение.}
Случайные величины бывают:
\begin{itemize}
	\item Дискретные(Множество значений конечно или счетно)
	\item Непрерывные(Принимают бесконечное, континуальное число значений)
\end{itemize}
{\bf Распределение дискретной случайной величины} - таблица, которая описывает, какие значения принимает случайная величина с какой вероятностью. Сумма вероятностей должна быть равна 1, каждая вероятность лежит между 0 и 1.
\\
{\bf Функция распределения} - функция, которая определяет вероятность события $X \leq x$, то есть
\[
F(X) = \mathbb{P}(X \leq x) = \sum \mathbb{P}(X=k)\cdot [X \leq x]
\]
\[
[X \leq x] = 
  \begin{cases}
     1 & [X \leq x] \\
     0 & otherwise
  \end{cases}
\]
\subsection{Непрерывные случайные величины}
\paragraph{Плотность распределения.} Распределение непрерывной случайной величины описывается плотностью распределения вероятноcтей.
\begin{figure}[h!]
  \centering
  \incfig{funcplotnost}
\end{figure}
\\
Площадь под всей плотностью должна быть равна 1.
\paragraph{Функция распределения.} Это функция, которая определяет вероятность события $X \leq x$, то есть
\[
F(x) = \PP(X \leq x) = \int_{-\infty}^{x}f(t)dt, f(t) - \; density \; function
\]
\begin{figure}[h!]
  \centering
  \incfig{funcpasp}
\end{figure}
\paragraph{Важные свойства.}
\begin{itemize}
	\item Плотность определениа только для непрерывных случайных величин. Для дискретных табличка.
	\item $f(x) = F'(x)$
    \item $\int_{-\infty}^{+\infty}f(t)dt=1, f(t) \geq 0 \; \forall t$
    \item $F(X)$ не убывает и лежит между 0 и 1
    \item $\PP(a \leq X \leq b) = \int_{a}^{b}f(t)dt=F(b)- F(a)$
    \item Вероятность того, что непрерывная случайная величина попадет в точку, равна 0.
\end{itemize}
\subsection{Характеристики случайных величин}
\paragraph{Математическое ожидание.}
Математическое ожидание - среднее значение случайной величины.
\[
\E(X)=\sum_{k=1}^{n}k \cdot \PP(X=k)
\]
\[
\E(X)=\int_{-\infty}^{+\infty}t \cdot f(t)dt
\]
\paragraph{Свойства мат ожидания.}
\begin{itemize}
    \item $\E(a)=a$
    \item $\E(X+Y)= \E(X) + \E(Y)$
    \item $\E(a \cdot X) = a \cdot \E(X)$
    \item $\E(X \cdot Y) = \E(X) \cdot \E(Y)$ , если независимы
    \item Математическое ожидание случайной величины не случайно.
    \item $\E(X-\E(X)) = 0$
\end{itemize}
\paragraph{Дисперсия.}
Дисперсия - мера разброса случайной величины вокруг её среднего.
\[
Var(X)=\E(X-\E(X))^2 = \sum_{k=1}^{n}(k-\E(X))^2 \cdot \PP(X=k) 
\]
\[
Var(X)=\E(X-\E(X))^2 = \int_{-\infty}^{+\infty}(t-\E(X))^2 \cdot f(t)dt 
\]
Более удобное её искать по формуле
\[
Var(X)=\E(X^2) - \E^2(X)
\]
\paragraph{Среднеквадратическое отклонение.}
\[
\sigma(X) = \sqrt{Var(X)}
\]
\paragraph{Свойства дисперсии.}
\begin{itemize}
    \item $Var(a)=0$
    \item $Var(X+Y)=Var(X)+Var(Y)$, если независимы
    \item $Var(a \cdot X)=a^2 \cdot Var(X)$
    \item $Var(X-Y)=Var(X)+Var(Y)$, если независимы
    \item Дисперсия случайной величины не случайна
\end{itemize}
\paragraph{Мода случайной величины.}
Мода случайной величины - значение, которому соответствует наибольшая вероятность(для дискретной величины) и локальный максимум плотности распределения(для непрерывной случайной величны). По-другому это значение, которое встречается во множестве наблюдений наиболее часто.
\paragraph{Медиана случайной величины.}
Медиана случайной величины - такое её значение, что
\[
\PP(X<Med(X))=\PP(X>Med(X)) = 0.5
\]
\begin{figure}[h!]
  \centering
  \incfig{median}
\end{figure}
\newpage
\paragraph{Квантиль уровня $\gamma$.}
Квантиль уровня $\gamma$ - это такое число $q$, что
\[
\PP(X\leq q)= \gamma
\]
\begin{figure}[h!]
  \centering
  \incfig{quantile}
\end{figure}
\subsection{Типы случайных величин}
\paragraph{Распределение Бернулли.} Случайная величина в нем принимает два значения 0 или 1. Например пол родившегося ребёнка.
Задается вероятностью успеха $p$
\[
X \sim Bern(p)
\]
\[
\E(X) = p
\]
\[
Var(X)=p \cdot (1-p)
\]
\paragraph{Биномиальное распределение.}
Число попаданий в баскетбольную корзину. Задается $n$ - число испытаний и $p$ - вероятность успеха.
\[
X \sim Bin(p,n)
\]
\[
\E(X) = n \cdot p 
\]
\[
Var(X)= n \cdot p \cdot (1-p)
\]
\[
\PP(X=k)={n \choose k} \cdot p^k(1-p)^{n-k}
\]
\paragraph{Геометрическое распределение.}
Номер броска, когда произошло первое попадание в корзину. Задается $p$ - вероятность успеха.
\[
X \sim Geom(P)
\]
\[
\E(X)=\frac{1}{p}
\]
\[
Var(X)=\frac{1-p}{p^2}
\]
\[
\PP(X=k)=p \cdot (1-p)^{k-1}
\]
\paragraph{Произвольное дискретное распределние.}
Подбрасывание игральной кости. Задаётся таблицей.
\paragraph{Распределение Пуассона.}
Число автобусов, проехавших за час мимо остановки. Задается $\lambda$ - интенсивность потока событий.
\[
X \sim Poiss(\lambda)
\]
\[
\PP(X=k)=\frac{\lambda^k \cdot e^{-\lambda}}{k!}
\]
\[
Var(X)=\lambda
\]
\[
\E(X)=\lambda
\]
\paragraph{Экспоненциальное распределение.}
Время до события. Нету памяти, время которое осталось ждать не зависит от того, сколько уже прошло.
\[
X \sim Exp(\lambda)
\]
\[
f(x)=\lambda \cdot e^{-\lambda \cdot x}, \; x \geq 0
\]
\[
F(x)=1-e^{-\lambda \cdot x}, \; x \geq 0
\]
\[
\E(X) = \frac{1}{\lambda}
\]
\[
Var(X)=\frac{1}{\lambda^2}
\]
\paragraph{Равномерное распределение.}
Время рождения ребёнка
\[
X \sim U[a;b]
\]
\[
f(x)=\frac{1}{b-a}, x \in [a;b]
\]
\[
F(x)=\frac{x-a}{b-a}, x \in [a;b]
\]
\[
\E(X)= \frac{a+b}{2}
\]
\[
Var(X)=\frac{(b-a)^2}{12}
\]
\paragraph{Нормальное распределение.}
Погрешность весов.
\[
X \sim N(\mu,\sigma^2)
\]
\[
\E(X) = \mu
\]
\[
Var(X)=\sigma^2
\]
\paragraph{Перцентиль.}
Перцентиль порядка $k$ - это такое число, что $k\%$ меьше этого числа. Квартили - перцентили с шагом 0.25. Интерквартильный размах
\[
IQR = x_{0.75}-x_{0.25} 
\]
\subsection{Эмпирическая тема}
\paragraph{Эмпирическая функция распределения.}
Эмпирическая функция распределения - функция, которая определяет для каждого $x$ частоту события $X \leq x$. Похоже на функцию распределения дискретной случайной величины.
\paragraph{Гистограмма.}
Гистограмма - эмпирическая оценка плотности распределения. По оси $x$ откладывают значения. По оси $y$ частоты. Область возможных значений обычно дробят на отрезки, бины. Чем короче бины тем детальнее рисуется гистограмма.
\section{Week 2}
\subsection{Независимость}
Говорят, что случайные величины $X$ и $Y$ независимы, если
\[
F(x,y)=F_X(x) \cdot F_Y(y)
\]
В терминах плотностей:
\[
f(x,y)=f_X(x) \cdot f_Y(y)
\]
\paragraph{Ковариация.}
Ковариация - мера линейной зависимости двух случайных величин
\[
Cov(X,Y) = \E[(X-\E(X)) \cdot Y-\E(Y)]
\]
По аналогии с дисперсией, более простая формула:
\[
Cov(X,Y) = \E(X \cdot Y) - \E(X) \cdot \E(Y)
\]
\paragraph{Свойства ковариации.}
\begin{itemize}
  \item $Cov(X,Y) = Cov(Y,X)$
  \item $Cov(a,b)=0$
  \item $Cov(a \cdot X,Y) = a \cdot Cov(X,Y)$
  \item $Cov(X+a,Y)=Cov(X,Y)$
  \item $Cov(X+Z,Y) = Cov(X,Y)+Cov(Z,Y)$
  \item $Cov(X,X)=Var(X)$
  \item Если случайные величины независимы: $Cov(X,Y)=0$
  \item Обратное неверно
  \item Если $X$ и $Y$ зависимы, то $\E(X,Y)=\E(X) \cdot \E(Y) + Cov(X,Y)$
  \item $Var(X+Y)=Var(X)+Var(Y) + 2 \cdot Cov(X,Y)$
\end{itemize}
\paragraph{Корреляция Пирсона.}
\[
\rho(X,Y) = \frac{Cov(X,Y)}{\sigma(X) \cdot \sigma(Y)}
\]
Коэффициент корреляции характеризует тесноту и направленность линейной связи между случайными величинами и принимает значение от -1 до 1. Корреляция Пирсона улавливает только линейную взаимосвязь и чувствительна к выбросам.
\newpage
\paragraph{Корреляция Спирмена.}
Корреляция Спирмена - мера силы монотонной взаимосвязи. Вычисляется как корреляция Пирсона между рангами наблюдений.
\begin{figure}[h!]
  \centering
  \incfig{spearmanexample}
\end{figure}
\\
Корреляция Спирмана пытается уловить в данных монотонность.
\paragraph{Нормальное распределение.}
Нормальная случайная величина
\begin{figure}[h]
  \centering
  \incfig{normaldistribution}
\end{figure}
\paragraph{Свойства нормального распределения.}
\begin{itemize}
  \item Распределение симметрично относительно точки $\E(X)$
  \item Параметр $\mu$ не влияет на форму кривой и отвечает за её сдвиг кривой вдоль оси $x$, параметр $\sigma$ определяет степень размытости кривой.
  \item $X+Y \sim N(\mu_x+\mu_y,\sigma_x^2 + \sigma_y^2)$
  \item $X+a \sim N(\mu_x + a, \sigma_x^2)$
  \item $a \cdot X \sim N(a\cdot \mu_x, a^2 \cdot \sigma_x^2)$
\end{itemize}
\paragraph{Центрирование и нормирование.}
\[
X \sim N(\mu,\sigma^2)
\]
Центрирование
\[
X - \mu \sim N(0,\sigma^2)
\]
Нормирование
\[
\frac{X - \mu}{\sqrt{\sigma^2}} \sim N(0,1)
\]
Распределение $N(0,1)$ называется {\bf стандартным нормальным распределением.} Для функции распределения случайной величины $N(0,1)$ составлены таблицы.
\\
{\bf Правила сигм.}
\begin{figure}[h!]
  \centering
  \incfig{sigmrule}
\end{figure}
\paragraph{Эксцесс и куртосис.}
Куртосис
\[
\frac{\E[(X-\E(X))^4]}{\sigma^4}
\]
Эксцессом случайной величины $X$ называют величину
\[
\beta_x=\frac{\E[(X-\E(X))^4]}{\sigma^4} -3
\]
Число 3 вычитается из курстосиса, чтобы эксцесс нормального распределения был равен 0. Если хвосты распределения легче, а пик острее, чем у нормального распределения, тогда $\beta_X>0$. Если хвосты распределения тяжелее, а пик более приплюснутый, тогда $\beta<0$.
\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
    \incfig{minusecscess}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \incfig{plusecscess}
  \end{subfigure}
\end{figure}
\paragraph{Коэффициент асимметрии (skewness).}
Коэффициентом ассиметрии случайной величины $X$ назвают величину
\[
A_x=\frac{\E[(X-\E(X))^3]}{\sigma^3}
\]
Если плотность распределения симметрична, то $A_X = 0$. Если левый хвост тяжелее, то $A_X>0$. Если праый хвост тяжелее, то $A_X<0$
\begin{figure}[h!]
  \centering
  \incfig{coefassymetry}
\end{figure}
\\
{\bf Многомерное нормальное распределение.}
\begin{figure}[h!]
  \centering
  \incfig{multidimnormaldist}
\end{figure}
\subsection{Ядерные оценки плотности.}
\paragraph{Гистограмма.}
Гистограмма простейший непараметрический способ получить оценку плотности распределения.
\[
f_n(x)=\frac{1}{n\cdot h} \cdot \sum [z_k < x_i \leq z_{k+1}]
\]
Размер бина(длина отрезка)
\[
h = z_{k+1}-z_k
\]
Добавим скользящие границы
\[
f_n(x)=\frac{1}{n\cdot h} \cdot \sum [x - \frac{h}{2} < x_i \leq x + \frac{h}{2}]
\]
Перепишем в более удобном виде
\[
f_n(x)=\frac{1}{n\cdot h} \cdot \sum K(\frac{x-x_i}{h}) \; \; \; \; K(z)=[-\frac{1}{2} <z \leq \frac{1}{2}]
\]
Такая функция придаёт каждому наблюдению вес 0 или 1. Функция $K(z)$ называется ядерная функция. Она должна быть неотрицательной и $\int K(Z)dz=1$ (сумма всех весов равна 1). Ядерные функции бывают разные, чаще всего используют Гауссовское ядро:
\begin{figure}[h!]
  \centering
  \incfig{gausscore}
\end{figure}
\newpage
\subsection{Пропуски в данных.}
\paragraph{Борьба с пропусками}
\begin{itemize}
  \item Простые методы: замена средним, медианой, модой
  \item Сложные методы: основаны на машинном обучении, смотрят на другие примеры и пытаются предсказать что было пропущено.
\end{itemize}
\paragraph{Выбросы.}
Выброс - результат измерений, который сильно выделяется на общем фоне. Многие алгоритмы чувствительны к выбросам и переобучаются под них. Можно воспользоваться {\bf правилом трёх сигм} и избавиться от всех выбросов, что выходят за границы. Так же существоет правило 1.5 интерквартильньных размахов (IQR): Если наблюдения оказались за пределами выделенного интервала, они выбросы. Иногда используют 3 IQR.
\begin{figure}[h!]
  \centering
  \incfig{1.5iqr}
\end{figure}
\paragraph{Преобразование данных.}
Логарифмирование
\begin{figure}[h!]
  \centering
  \incfig{log}
\end{figure}
\\
Логирафм обладает затухающим свойством, поэтому разница больших величин не такая большая как разница маленьких.
\begin{figure}[h!]
  \centering
  \incfig{log1}
\end{figure}
\\
Сгустки в начале оси абсцисс стали распределены более равномерно из-за того, что там логарифм растет быстрее. Расстояние между точками с большими значениями стало меньше, так как там логарифм растет медленее.
\paragraph{Преобразование Бокса-Кокса.}
Иногда скорость роста дефолтного логарифма не подходит. Нам поможет преобразование Бокса-Кокса. Параметр $p$ надо подобрать.
\begin{figure}[h!]
  \centering
  \incfig{boxkoks}
\end{figure}
\newpage
\subsection{Масштабироваание и категориальные переменные}
{\bf Способы масштабирования.}
\begin{figure}[h!]
  \centering
  \incfig{zooming}
\end{figure}
\paragraph{Категориальные переменные.}
One-hot encoding(бинарное кодирование) топ, главное не попасть в {\bf Dummy-ловушку}. Dummy-ловушка - ситуация, когда набор закодированных столбцов в сумме даёт колонку из единиц. Возникает линейная зависимость и методы плохо работают. Поэтому надо всегда $drop\_first=True$. Количество столбцов должно быть $k-1$ от количества уникальных значений категориального признака.
\section{Week 5}
\subsection{Распределение хи-квадрат}
Случайные величины $X_1,...,X_k \sim iid \; N(0,1).$
\\
Случайная величина $Y=X_1^2 + ... + X_k^2 \sim \chi_k^2$ имеет 'хи-квадрат' распределение с $k$ степенями свободы. (iid - identically independently distributed)
\begin{figure}[h!]
  \centering
  \incfig{xisquared}
\end{figure}
\paragraph{Число степеней свободы.}
Число степеней свободы - количество элементов варьирования, которые могут принимать произвольные значения, не изменяющие заданных характеристик.
\subsection{Распределение Стьдента}
Независимые случайные величины $X_0 \sim N(0,1), \; Y\sim \chi_k^2$. Тогда случайная величина
\[
Z=\frac{X_0}{\sqrt{Y/k}} \sim t(k)
\]
имеет распределение Стьюдента с $k$ степенями свободы.
\begin{figure}[h!]
  \centering
  \incfig{studentsdist}
\end{figure}
\\
Распределение Стьюдента обладает более тяжелыми хвостами, нежели нормальное. По мере возрастания $k$, кривая функции плотности все больше напоминает стандартное нормальное распределение.
\subsection{Распределение Фишера}
Независимые случайные величины $X\sim \chi_k^2,X\sim \chi_m^2$. Случайная величина
\[
Z = \frac{X/k}{Y/m} \sim F(k,m)
\]
имеет распределение Фишера с $k,m$ степенями свободы.
\begin{figure}[h!]
  \centering
  \incfig{fishdist}
\end{figure}
\paragraph{Вывод.}
Распределения хи-квадрат, Стьюдента, Фишера часто встречаются на практике при анализе нормально распределённых выборок.
\section{Закон больших чисел}
ЗБЧ говорит, что среднее арифметическое большого числа похожих случайных величин 'стабилизируется' с ростом их числа.
\paragraph{Слабая форма ЗБЧ.} Пусть $X_1,...,X_n$ попарно независимые и одинакого распределённые случайные величины с конечной дисперсией $Var(X_1)<\infty$ тогда:
\[
{\overline X} = \frac{X_1+...+X_n}{n} \xrightarrow{p} \E(X_1)
\]
Среднее сходится по вероятности к математическому ожиданию при $n \rightarrow \infty$
\paragraph{Сходимость по вероятности.}
Последовательность случайных величин $X_1,...,X_n,...$ сходится по вероятности к случайной величине $X$, если
\[
\forall \varepsilon > 0 \; \PP(|X_n - X|<\varepsilon) \rightarrow 1 \; when \; n \rightarrow \infty
\]
То есть:
\[
\lim_{n \rightarrow \infty} \PP(|X_n - X|<\varepsilon) = 1
\]
{\bf Свойства сходимости по вероятности}
\begin{figure}[h!]
  \centering
  \incfig{plimfeatures}
\end{figure}
\subsection{Центральная предельная теорема}
ЦПТ говорит, что сумма довольно большого числа случайных величин имеет распределение близкое к нормальному.
\\
Пусть $X1,...,X_n$ попарно независимые и одинакого распределённые случайные величины с конечной дисперсией $Var(X_1)<\infty$ тогда: 
\[
\frac{X_1 + ...+X_n}{n} \xrightarrow{d} N(\E(X_1),\frac{Var(X_1)}{n})
\]
$d$ над стрелкой означает сходимость по распределению.
\paragraph{Cходимость по распределению.}
Последовательность случайных величин $X_1,...,X_n,...$ сходится по распределению к случайной величине $X$, если 
\[
\lim_{n \rightarrow \infty} F_{X_n}(x)=F_x(x),
\]
то есть последовательность функций распределения $F_{X_n}(x)$ сходится к функции $F_x(x)$ во всех точках $x$, где $F_x(x)$ непрерывна.
\newpage
{\bf ЗБЧ vs. ЦПТ}
\begin{figure}[h!]
  \centering
  \incfig{versus}
\end{figure}
\subsection{Виды сходимостей}
\begin{figure}[h!]
  \centering
  \incfig{convtypes}
\end{figure}
\paragraph{Сходимость почти наверное.}
Последовательность случайных величин $X_1,...,X_n,...$ сходится почти наверное (с вероятностью единица) к случайной величине $X$, если 
\[
\PP(\lim_{n \rightarrow \infty} X_n = X) = 1,
\]
то есть у последовательности есть предел с вероятностью 1.
\paragraph{Сильная форма ЗБЧ.} Пусть $X_1,...,X_n$ последовательность независимых и одинакого распределённых случайных величин с $\E(|X_1|)<\infty$ тогда:
\[
{\overline X} = \frac{X_1+...+X_n}{n} \xrightarrow{a.s.} \E(X_1)
\]
Среднее сходится по почти наверное к математическому ожиданию при $n \rightarrow \infty$
\end{document}
